TASK 1: Load and Inspect Model
======================================================================
Model name: distilgpt2
Number of layers (n_layer): 6
Hidden size (n_embd): 768
Attention heads (n_head): 12
Vocabulary size: 50257
Max position embeddings: 1024

Running warmup passes to stabilize GPU performance...
Warmup complete.

TASK 2: Generation Without KV Cache
======================================================================

Generating 32 tokens without cache (3 runs)...
  Sequence length: 32
  Average latency: 4.48 ms/token
  Peak memory used: 13.33 MB

Generating 64 tokens without cache (3 runs)...
  Sequence length: 64
  Average latency: 4.48 ms/token
  Peak memory used: 25.70 MB

Generating 128 tokens without cache (3 runs)...
  Sequence length: 128
  Average latency: 4.48 ms/token
  Peak memory used: 51.31 MB

Generating 256 tokens without cache (3 runs)...
  Sequence length: 256
  Average latency: 4.58 ms/token
  Peak memory used: 100.76 MB

----------------------------------------------------------------------
Summary: Without KV Cache
----------------------------------------------------------------------
Seq Length      Latency (ms/token)   Memory (MB)    
--------------------------------------------------
32              4.48                 13.33          
64              4.48                 25.70          
128             4.48                 51.31          
256             4.58                 100.76         

TASK 3: Generation With KV Cache
======================================================================

Generating 32 tokens with cache (3 runs)...
  Sequence length: 32
  Average latency: 4.52 ms/token
  Peak memory used: 2.81 MB
  KV cache size: 1.2305 MB

Generating 64 tokens with cache (3 runs)...
  Sequence length: 64
  Average latency: 4.52 ms/token
  Peak memory used: 5.06 MB
  KV cache size: 2.3555 MB

Generating 128 tokens with cache (3 runs)...
  Sequence length: 128
  Average latency: 4.52 ms/token
  Peak memory used: 9.56 MB
  KV cache size: 4.6055 MB

Generating 256 tokens with cache (3 runs)...
  Sequence length: 256
  Average latency: 4.52 ms/token
  Peak memory used: 18.57 MB
  KV cache size: 9.1055 MB

----------------------------------------------------------------------
Summary: With KV Cache
----------------------------------------------------------------------
Seq Len    Latency (ms)    Memory (MB)     KV Cache (MB)  
-------------------------------------------------------
32         4.52            2.81            1.2305         
64         4.52            5.06            2.3555         
128        4.52            9.56            4.6055         
256        4.52            18.57           9.1055         

COMPARISON: Speedup Analysis
======================================================================
Seq Len    No Cache (ms)   With Cache (ms)    Speedup   
-------------------------------------------------------
32         4.48            4.52               0.99      x
64         4.48            4.52               0.99      x
128        4.48            4.52               0.99      x
256        4.58            4.52               1.01      x

TASK 4: Memory Analysis
======================================================================

KV Cache Memory Formula:
  M_KV = 2 × L × D × N_layers × sizeof(dtype)
  Where: D = 768, N_layers = 6, dtype = float32 (4 bytes)

Seq Len    Theoretical (MB)   Measured (MB)      Difference     
------------------------------------------------------------
32         1.2656             1.2305             -2.78%
64         2.3906             2.3555             -1.47%
128        4.6406             4.6055             -0.76%
256        9.1406             9.1055             -0.38%

TASK 5 (Bonus): Quantized KV Cache Analysis
======================================================================
Note: bitsandbytes not installed. Showing theoretical analysis only.

Quantization reduces KV cache memory by storing in int8 instead of float32.
Theoretical memory savings: 4x (32-bit -> 8-bit)

Seq Len    FP32 Cache (MB)    INT8 Cache (MB)    Savings        
------------------------------------------------------------
32         1.2305             0.3076             75.0%
64         2.3555             0.5889             75.0%
128        4.6055             1.1514             75.0%
256        9.1055             2.2764             75.0%

======================================================================
TRADE-OFFS: Latency vs Memory
======================================================================

The KV cache presents a classic space-time trade-off:

WITHOUT KV CACHE:
  - Latency: O(L²) complexity - must recompute all attention at each step
  - Memory: Lower base memory, but activations grow with sequence length
  - Observed: 13.33 MB (L=32) → 100.76 MB (L=256) peak memory

WITH KV CACHE:
  - Latency: O(L) complexity - only compute attention for new token
  - Memory: Must store K and V tensors for all previous tokens
  - Observed: 2.81 MB (L=32) → 18.57 MB (L=256) peak memory

TRADE-OFF ANALYSIS:
  - KV caching uses additional memory (9.1 MB at L=256) to store cached values
  - In return, it avoids redundant computation of previous tokens' attention
  - For distilgpt2, the memory savings (5x) outweigh the cache overhead
  - For larger models (GPT-3, LLaMA), latency savings become dramatic
  - The trade-off favors KV caching for most inference scenarios

WHEN TO USE KV CACHE:
  - Always for autoregressive generation (chatbots, text completion)
  - When memory is available to store the cache
  - Especially beneficial for long sequences and large models

WHEN TO AVOID KV CACHE:
  - Single-pass encoding tasks (embeddings, classification)
  - Extremely memory-constrained environments
  - Very short sequences where overhead exceeds benefit

======================================================================
FINAL SUMMARY
======================================================================

Key Findings:

1. Latency is similar (~4.5 ms/token) for both methods at these sequence lengths.
   - distilgpt2 is small enough that the GPU processes both methods equally fast.
   - The theoretical O(L²) vs O(L) complexity difference becomes visible with 
     larger models or longer sequences.

2. Memory reduction with KV cache is significant (5x):
   - Without cache: 13.33 MB → 100.76 MB (grows quadratically)
   - With cache: 2.81 MB → 18.57 MB (grows linearly with KV cache)

3. KV cache size matches theoretical prediction within 3%:
   - Formula: M_KV = 2 × L × D × N_layers × 4 bytes
   - This validates our understanding of cache memory requirements.

4. Quantization (INT8) could further reduce KV cache memory by 75%.

5. For small models like distilgpt2, KV caching primarily saves memory.
   Latency benefits become more pronounced with larger models or longer sequences.